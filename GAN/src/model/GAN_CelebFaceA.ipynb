{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_CelebFaceA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/siddharthalodha/DeepLearningImplementations/blob/master/GAN/src/model/GAN_CelebFaceA.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "K70hAckqg0EA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TsO9WacsQbMC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#os.listdir(\"/Colorful/data/raw\")\n",
        "#os.listdir(\"/Colorful/data/raw/img_align_celeba/img_align_celeba\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hH5ebnAGSCyE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive\n",
        "\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "fileId = drive.CreateFile({'id': '1cUBPxqU-9Y6f_OAfPwdRmPg7ruZ2sfdk'})\n",
        "print(fileId['title'])  # CelebA Dataset\n",
        "fileId.GetContentFile('CelebA_64_data.h5')  # Save Drive file as a local file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ElbiB72ccM4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BUUUOixghb46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Data utils.py\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "\n",
        "def normalization(X):\n",
        "\n",
        "    return X / 127.5 - 1\n",
        "\n",
        "\n",
        "def inverse_normalization(X):\n",
        "\n",
        "    return (X + 1.) / 2.\n",
        "\n",
        "\n",
        "def load_mnist(image_data_format):\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    if image_data_format == 'channels_first':\n",
        "        X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
        "        X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "    else:\n",
        "        X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train = normalization(X_train)\n",
        "    X_test = normalization(X_test)\n",
        "\n",
        "    nb_classes = len(np.unique(np.hstack((y_train, y_test))))\n",
        "\n",
        "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "\n",
        "def load_celebA(img_dim, image_data_format):\n",
        "\n",
        "    with h5py.File(\"../../data/processed/CelebA_%s_data.h5\" % img_dim, \"r\") as hf:\n",
        "\n",
        "        X_real_train = hf[\"data\"][:].astype(np.float32)\n",
        "        X_real_train = normalization(X_real_train)\n",
        "\n",
        "        if image_data_format == \"channels_last\":\n",
        "            X_real_train = X_real_train.transpose(0, 2, 3, 1)\n",
        "\n",
        "        return X_real_train\n",
        "\n",
        "\n",
        "def gen_batch(X, batch_size):\n",
        "\n",
        "    while True:\n",
        "        idx = np.random.choice(X.shape[0], batch_size, replace=False)\n",
        "        yield X[idx]\n",
        "\n",
        "\n",
        "def sample_noise(noise_scale, batch_size, noise_dim):\n",
        "\n",
        "    return np.random.normal(scale=noise_scale, size=(batch_size, noise_dim[0]))\n",
        "\n",
        "\n",
        "def get_disc_batch(X_real_batch, generator_model, batch_counter, batch_size, noise_dim,\n",
        "                   noise_scale=0.5, label_smoothing=False, label_flipping=0):\n",
        "\n",
        "    # Create X_disc: alternatively only generated or real images\n",
        "    if batch_counter % 2 == 0:\n",
        "        # Pass noise to the generator\n",
        "        noise_input = sample_noise(noise_scale, batch_size, noise_dim)\n",
        "        # Produce an output\n",
        "        X_disc = generator_model.predict(noise_input)\n",
        "        y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
        "        y_disc[:, 0] = 1\n",
        "\n",
        "        if label_flipping > 0:\n",
        "            p = np.random.binomial(1, label_flipping)\n",
        "            if p > 0:\n",
        "                y_disc[:, [0, 1]] = y_disc[:, [1, 0]]\n",
        "\n",
        "    else:\n",
        "        X_disc = X_real_batch\n",
        "        y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
        "        if label_smoothing:\n",
        "            y_disc[:, 1] = np.random.uniform(low=0.9, high=1, size=y_disc.shape[0])\n",
        "        else:\n",
        "            y_disc[:, 1] = 1\n",
        "\n",
        "        if label_flipping > 0:\n",
        "            p = np.random.binomial(1, label_flipping)\n",
        "            if p > 0:\n",
        "                y_disc[:, [0, 1]] = y_disc[:, [1, 0]]\n",
        "\n",
        "    return X_disc, y_disc\n",
        "\n",
        "\n",
        "def get_disc_batch_mixed(X_real_batch, generator_model, batch_counter, batch_size, noise_dim, noise_scale=0.5):\n",
        "\n",
        "    # Pass noise to the generator\n",
        "    noise_input = sample_noise(noise_scale, batch_size / 2, noise_dim)\n",
        "    # Produce an output\n",
        "    X_disc_noise = generator_model.predict(noise_input)\n",
        "    y_disc_noise = np.zeros((X_disc_noise.shape[0], 2), dtype=np.uint8)\n",
        "    y_disc_noise[:, 0] = 1\n",
        "\n",
        "    X_disc = X_real_batch[:batch_size / 2]\n",
        "    y_disc = np.zeros((X_disc.shape[0], 2), dtype=np.uint8)\n",
        "    y_disc[:, 1] = 1\n",
        "\n",
        "    X_disc = np.concatenate((X_disc, X_disc_noise))\n",
        "    y_disc = np.concatenate((y_disc, y_disc_noise))\n",
        "\n",
        "    return X_disc, y_disc\n",
        "\n",
        "\n",
        "def get_gen_batch(batch_size, noise_dim, noise_scale=0.5):\n",
        "\n",
        "    X_gen = sample_noise(noise_scale, batch_size, noise_dim)\n",
        "    y_gen = np.zeros((X_gen.shape[0], 2), dtype=np.uint8)\n",
        "    y_gen[:, 1] = 1\n",
        "\n",
        "    return X_gen, y_gen\n",
        "\n",
        "\n",
        "def plot_generated_batch(X_real, generator_model, batch_size, noise_dim, image_data_format, noise_scale=0.5):\n",
        "\n",
        "    # Generate images\n",
        "    X_gen = sample_noise(noise_scale, batch_size, noise_dim)\n",
        "    X_gen = generator_model.predict(X_gen)\n",
        "\n",
        "    X_real = inverse_normalization(X_real)\n",
        "    X_gen = inverse_normalization(X_gen)\n",
        "\n",
        "    Xg = X_gen[:8]\n",
        "    Xr = X_real[:8]\n",
        "\n",
        "    if image_data_format == \"channels_last\":\n",
        "        X = np.concatenate((Xg, Xr), axis=0)\n",
        "        list_rows = []\n",
        "        for i in range(int(X.shape[0] / 4)):\n",
        "            Xr = np.concatenate([X[k] for k in range(4 * i, 4 * (i + 1))], axis=1)\n",
        "            list_rows.append(Xr)\n",
        "\n",
        "        Xr = np.concatenate(list_rows, axis=0)\n",
        "\n",
        "    if image_data_format == \"channels_first\":\n",
        "        X = np.concatenate((Xg, Xr), axis=0)\n",
        "        list_rows = []\n",
        "        for i in range(int(X.shape[0] / 4)):\n",
        "            Xr = np.concatenate([X[k] for k in range(4 * i, 4 * (i + 1))], axis=2)\n",
        "            list_rows.append(Xr)\n",
        "\n",
        "        Xr = np.concatenate(list_rows, axis=1)\n",
        "        Xr = Xr.transpose(1,2,0)\n",
        "\n",
        "    if Xr.shape[-1] == 1:\n",
        "        plt.imshow(Xr[:, :, 0], cmap=\"gray\")\n",
        "    else:\n",
        "        plt.imshow(Xr)\n",
        "    plt.savefig(\"../../figures/current_batch.png\")\n",
        "    plt.clf()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MoUUCFTuhnAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Batch_utils.py\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import os\n",
        "import h5py\n",
        "import matplotlib.pylab as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.pyplot import cm\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "    \"\"\"\n",
        "    Generate minibatches with real-time data parallel augmentation on CPU\n",
        "    args :\n",
        "        hdf5_file   (str)      path to data in HDF5 format\n",
        "        batch_size  (int)      Minibatch size\n",
        "        dset        (str)      train/test/valid, the name of the dset to iterate over\n",
        "        maxproc     (int)      max number of processes to spawn in parallel\n",
        "        num_cached  (int)      max number of batches to keep in queue\n",
        "    yields :\n",
        "         X, y (minibatch data and labels as np arrays)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hdf5_file,\n",
        "                 batch_size=32,\n",
        "                 nb_classes=12,\n",
        "                 dset=\"training\",\n",
        "                 maxproc=8,\n",
        "                 num_cached=10):\n",
        "\n",
        "        # Check file exists\n",
        "        assert os.path.isfile(hdf5_file), hdf5_file + \" doesn't exist\"\n",
        "\n",
        "        # Initialize class internal variables\n",
        "        self.dset = dset\n",
        "        self.maxproc = maxproc\n",
        "        self.hdf5_file = hdf5_file\n",
        "        self.batch_size = batch_size\n",
        "        self.num_cached = num_cached\n",
        "        self.nb_classes = nb_classes\n",
        "\n",
        "        # Dict that will store all transformations and their parameters\n",
        "        self.d_transform = {}\n",
        "\n",
        "        # Read the data file to get dataset shape information\n",
        "        with h5py.File(self.hdf5_file, \"r\") as hf:\n",
        "            self.X_shape = hf[\"data\"].shape\n",
        "            assert len(self.X_shape) == 4,\\\n",
        "                (\"\\n\\nImg data should be formatted as: \\n\"\n",
        "                 \"(n_samples, n_channels, Height, Width)\")\n",
        "            self.n_samples = hf[\"data\"].shape[0]\n",
        "            # Verify n_channels is at index 1\n",
        "            assert self.X_shape[-3] < min(self.X_shape[-2:]),\\\n",
        "                (\"\\n\\nImg data should be formatted as: \\n\"\n",
        "                 \"(n_samples, n_channels, Height, Width)\")\n",
        "\n",
        "        # Save the class internal variables to a config dict\n",
        "        self.d_config = {}\n",
        "        self.d_config[\"hdf5_file\"] = hdf5_file\n",
        "        self.d_config[\"batch_size\"] = batch_size\n",
        "        self.d_config[\"dset\"] = dset\n",
        "        self.d_config[\"num_cached\"] = num_cached\n",
        "        self.d_config[\"maxproc\"] = maxproc\n",
        "        self.d_config[\"data_shape\"] = self.X_shape\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        return self.d_config\n",
        "\n",
        "    def gen_batch_inmemory_GAN(self, X_real, batch_size=None):\n",
        "        \"\"\"Generate batch, assuming X is loaded in memory in the main program\"\"\"\n",
        "\n",
        "        while True:\n",
        "\n",
        "            bs = self.batch_size\n",
        "            if batch_size is not None:\n",
        "                bs = batch_size\n",
        "\n",
        "            # Select idx at random for the batch\n",
        "            idx = np.random.choice(X_real.shape[0], bs, replace=False)\n",
        "            X_batch_real = X_real[idx]\n",
        "\n",
        "            yield X_batch_real"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUPZzBinhYda",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#General utils.py\n",
        "import os\n",
        "\n",
        "\n",
        "def remove_files(files):\n",
        "    \"\"\"\n",
        "    Remove files from disk\n",
        "    args: files (str or list) remove all files in 'files'\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(files, (list, tuple)):\n",
        "        for f in files:\n",
        "            if os.path.isfile(os.path.expanduser(f)):\n",
        "                os.remove(f)\n",
        "    elif isinstance(files, str):\n",
        "        if os.path.isfile(os.path.expanduser(files)):\n",
        "            os.remove(files)\n",
        "\n",
        "\n",
        "def create_dir(dirs):\n",
        "    \"\"\"\n",
        "    Create directory\n",
        "    args: dirs (str or list) create all dirs in 'dirs'\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(dirs, (list, tuple)):\n",
        "        for d in dirs:\n",
        "            if not os.path.exists(os.path.expanduser(d)):\n",
        "                os.makedirs(d)\n",
        "    elif isinstance(dirs, str):\n",
        "        if not os.path.exists(os.path.expanduser(dirs)):\n",
        "            os.makedirs(dirs)\n",
        "\n",
        "\n",
        "def setup_logging(model_name):\n",
        "\n",
        "    model_dir = \"../../models\"\n",
        "    # Output path where we store experiment log and weights\n",
        "    model_dir = os.path.join(model_dir, model_name)\n",
        "\n",
        "    fig_dir = \"../../figures\"\n",
        "\n",
        "    # Create if it does not exist\n",
        "    create_dir([model_dir, fig_dir])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "luzh9Wm2ccMp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Activation, Lambda, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Deconv2D, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers import Input, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def generator_upsampling(noise_dim, img_dim, bn_mode, model_name=\"generator_upsampling\", dset=\"celebA\"):\n",
        "    \"\"\"\n",
        "    Generator model of the DCGAN\n",
        "    args : img_dim (tuple of int) num_chan, height, width\n",
        "           pretr_weights_file (str) file holding pre trained weights\n",
        "    returns : model (keras NN) the Neural Net model\n",
        "    \"\"\"\n",
        "\n",
        "    s = img_dim[1]\n",
        "    f = 512\n",
        "\n",
        "    if dset == \"mnist\":\n",
        "        start_dim = int(s / 4)\n",
        "        nb_upconv = 2\n",
        "    else:\n",
        "        start_dim = int(s / 16)\n",
        "        nb_upconv = 4\n",
        "\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        bn_axis = 1\n",
        "        reshape_shape = (f, start_dim, start_dim)\n",
        "        output_channels = img_dim[0]\n",
        "    else:\n",
        "        reshape_shape = (start_dim, start_dim, f)\n",
        "        bn_axis = -1\n",
        "        output_channels = img_dim[-1]\n",
        "\n",
        "    gen_input = Input(shape=noise_dim, name=\"generator_input\")\n",
        "\n",
        "    x = Dense(f * start_dim * start_dim, input_dim=noise_dim)(gen_input)\n",
        "    x = Reshape(reshape_shape)(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    # Upscaling blocks\n",
        "    for i in range(nb_upconv):\n",
        "        x = UpSampling2D(size=(2, 2))(x)\n",
        "        nb_filters = int(f / (2 ** (i + 1)))\n",
        "        x = Conv2D(nb_filters, (3, 3), padding=\"same\")(x)\n",
        "        x = BatchNormalization(axis=1)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = Conv2D(nb_filters, (3, 3), padding=\"same\")(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(output_channels, (3, 3), name=\"gen_Conv2D_final\", padding=\"same\", activation='tanh')(x)\n",
        "\n",
        "    generator_model = Model(inputs=[gen_input], outputs=[x], name=model_name)\n",
        "\n",
        "    return generator_model\n",
        "\n",
        "\n",
        "def generator_deconv(noise_dim, img_dim, bn_mode, batch_size, model_name=\"generator_deconv\", dset=\"mnist\"):\n",
        "    \"\"\"\n",
        "    Generator model of the DCGAN\n",
        "    args : nb_classes (int) number of classes\n",
        "           img_dim (tuple of int) num_chan, height, width\n",
        "           pretr_weights_file (str) file holding pre trained weights\n",
        "    returns : model (keras NN) the Neural Net model\n",
        "    \"\"\"\n",
        "\n",
        "    assert K.backend() == \"tensorflow\", \"Deconv not implemented with theano\"\n",
        "\n",
        "    s = img_dim[1]\n",
        "    f = 512\n",
        "\n",
        "    if dset == \"mnist\":\n",
        "        start_dim = int(s / 4)\n",
        "        nb_upconv = 2\n",
        "    else:\n",
        "        start_dim = int(s / 16)\n",
        "        nb_upconv = 4\n",
        "\n",
        "    reshape_shape = (start_dim, start_dim, f)\n",
        "    bn_axis = -1\n",
        "    output_channels = img_dim[-1]\n",
        "\n",
        "    gen_input = Input(shape=noise_dim, name=\"generator_input\")\n",
        "\n",
        "    x = Dense(f * start_dim * start_dim, input_dim=noise_dim)(gen_input)\n",
        "    x = Reshape(reshape_shape)(x)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    # Transposed conv blocks\n",
        "    for i in range(nb_upconv - 1):\n",
        "        nb_filters = int(f / (2 ** (i + 1)))\n",
        "        s = start_dim * (2 ** (i + 1))\n",
        "        o_shape = (batch_size, s, s, nb_filters)\n",
        "        x = Deconv2D(nb_filters, (3, 3), output_shape=o_shape, strides=(2, 2), padding=\"same\")(x)\n",
        "        x = BatchNormalization(axis=-1)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "\n",
        "    # Last block\n",
        "    s = start_dim * (2 ** (nb_upconv))\n",
        "    o_shape = (batch_size, s, s, output_channels)\n",
        "    x = Deconv2D(output_channels, (3, 3), output_shape=o_shape, strides=(2, 2), padding=\"same\")(x)\n",
        "    x = Activation(\"tanh\")(x)\n",
        "\n",
        "    generator_model = Model(inputs=[gen_input], outputs=[x], name=model_name)\n",
        "\n",
        "    return generator_model\n",
        "\n",
        "\n",
        "def DCGAN_discriminator(noise_dim, img_dim, bn_mode, model_name=\"DCGAN_discriminator\", dset=\"mnist\", use_mbd=False):\n",
        "    \"\"\"\n",
        "    Discriminator model of the DCGAN\n",
        "    args : img_dim (tuple of int) num_chan, height, width\n",
        "           pretr_weights_file (str) file holding pre trained weights\n",
        "    returns : model (keras NN) the Neural Net model\n",
        "    \"\"\"\n",
        "\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "        bn_axis = 1\n",
        "    else:\n",
        "        bn_axis = -1\n",
        "\n",
        "    disc_input = Input(shape=img_dim, name=\"discriminator_input\")\n",
        "\n",
        "    if dset == \"mnist\":\n",
        "        list_f = [128]\n",
        "\n",
        "    else:\n",
        "        list_f = [64, 128, 256]\n",
        "\n",
        "    # First conv\n",
        "    x = Conv2D(32, (3, 3), strides=(2, 2), name=\"disc_Conv2D_1\", padding=\"same\")(disc_input)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Next convs\n",
        "    for i, f in enumerate(list_f):\n",
        "        name = \"disc_Conv2D_%s\" % (i + 2)\n",
        "        x = Conv2D(f, (3, 3), strides=(2, 2), name=name, padding=\"same\")(x)\n",
        "        x = BatchNormalization(axis=bn_axis)(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    def minb_disc(x):\n",
        "        diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
        "        abs_diffs = K.sum(K.abs(diffs), 2)\n",
        "        x = K.sum(K.exp(-abs_diffs), 2)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def lambda_output(input_shape):\n",
        "        return input_shape[:2]\n",
        "\n",
        "    num_kernels = 100\n",
        "    dim_per_kernel = 5\n",
        "\n",
        "    M = Dense(num_kernels * dim_per_kernel, use_bias=False, activation=None)\n",
        "    MBD = Lambda(minb_disc, output_shape=lambda_output)\n",
        "\n",
        "    if use_mbd:\n",
        "        x_mbd = M(x)\n",
        "        x_mbd = Reshape((num_kernels, dim_per_kernel))(x_mbd)\n",
        "        x_mbd = MBD(x_mbd)\n",
        "        x = Concatenate(axis=bn_axis)([x, x_mbd])\n",
        "\n",
        "    x = Dense(2, activation='softmax', name=\"disc_dense_2\")(x)\n",
        "\n",
        "    discriminator_model = Model(inputs=[disc_input], outputs=[x], name=model_name)\n",
        "\n",
        "    return discriminator_model\n",
        "\n",
        "\n",
        "def DCGAN(generator, discriminator_model, noise_dim, img_dim):\n",
        "\n",
        "    noise_input = Input(shape=noise_dim, name=\"noise_input\")\n",
        "\n",
        "    generated_image = generator(noise_input)\n",
        "    DCGAN_output = discriminator_model(generated_image)\n",
        "\n",
        "    DCGAN = Model(inputs=[noise_input],\n",
        "                  outputs=[DCGAN_output],\n",
        "                  name=\"DCGAN\")\n",
        "\n",
        "    return DCGAN\n",
        "\n",
        "\n",
        "def load(model_name, noise_dim, img_dim, bn_mode, batch_size, dset=\"mnist\", use_mbd=False):\n",
        "\n",
        "    if model_name == \"generator_upsampling\":\n",
        "        model = generator_upsampling(noise_dim, img_dim, bn_mode, model_name=model_name, dset=dset)\n",
        "        model.summary()\n",
        "        from keras.utils import plot_model\n",
        "        plot_model(model, to_file='../../figures/%s.png' % model_name, show_shapes=True, show_layer_names=True)\n",
        "        return model\n",
        "    if model_name == \"generator_deconv\":\n",
        "        model = generator_deconv(noise_dim, img_dim, bn_mode, batch_size, model_name=model_name, dset=dset)\n",
        "        model.summary()\n",
        "        from keras.utils import plot_model\n",
        "        plot_model(model, to_file='../../figures/%s.png' % model_name, show_shapes=True, show_layer_names=True)\n",
        "        return model\n",
        "    if model_name == \"DCGAN_discriminator\":\n",
        "        model = DCGAN_discriminator(noise_dim, img_dim, bn_mode, model_name=model_name, dset=dset, use_mbd=use_mbd)\n",
        "        model.summary()\n",
        "        from keras.utils import plot_model\n",
        "        plot_model(model, to_file='../../figures/%s.png' % model_name, show_shapes=True, show_layer_names=True)\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JYvIBjcLcg_F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import models_GAN as models\n",
        "from keras.utils import generic_utils\n",
        "from keras.optimizers import Adam, SGD\n",
        "# Utils\n",
        "sys.path.append(\"../utils\")\n",
        "import general_utils\n",
        "import data_utils\n",
        "\n",
        "\n",
        "def train(batch_size,n_batch_per_epoch,nb_epoch,generator,model_name,image_data_format,img_dim,bn_mode,label_smoothing,label_flipping,noise_scale,dset,use_mbd,epoch_size\n",
        "):\n",
        "    \"\"\"\n",
        "    Train model\n",
        "    Load the whole train data in memory for faster operations\n",
        "    args: **kwargs (dict) keyword arguments that specify the model hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # Roll out the parameters\n",
        "\n",
        "    # Setup environment (logging directory etc)\n",
        "    general_utils.setup_logging(model_name)\n",
        "\n",
        "    # Load and rescale data\n",
        "    if dset == \"celebA\":\n",
        "        X_real_train = data_utils.load_celebA(img_dim, image_data_format)\n",
        "    if dset == \"mnist\":\n",
        "        X_real_train, _, _, _ = data_utils.load_mnist(image_data_format)\n",
        "    img_dim = X_real_train.shape[-3:]\n",
        "    noise_dim = (100,)\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Create optimizers\n",
        "        opt_dcgan = Adam(lr=1E-3, beta_1=0.5, beta_2=0.999, epsilon=1e-08)\n",
        "        opt_discriminator = SGD(lr=1E-3, momentum=0.9, nesterov=True)\n",
        "\n",
        "        # Load generator model\n",
        "        generator_model = models.load(\"generator_%s\" % generator,\n",
        "                                      noise_dim,\n",
        "                                      img_dim,\n",
        "                                      bn_mode,\n",
        "                                      batch_size,\n",
        "                                      dset=dset,\n",
        "                                      use_mbd=use_mbd)\n",
        "        # Load discriminator model\n",
        "        discriminator_model = models.load(\"DCGAN_discriminator\",\n",
        "                                          noise_dim,\n",
        "                                          img_dim,\n",
        "                                          bn_mode,\n",
        "                                          batch_size,\n",
        "                                          dset=dset,\n",
        "                                          use_mbd=use_mbd)\n",
        "\n",
        "        generator_model.compile(loss='mse', optimizer=opt_discriminator)\n",
        "        discriminator_model.trainable = False\n",
        "\n",
        "        DCGAN_model = models.DCGAN(generator_model,\n",
        "                                   discriminator_model,\n",
        "                                   noise_dim,\n",
        "                                   img_dim)\n",
        "\n",
        "        loss = ['binary_crossentropy']\n",
        "        loss_weights = [1]\n",
        "        DCGAN_model.compile(loss=loss, loss_weights=loss_weights, optimizer=opt_dcgan)\n",
        "\n",
        "        discriminator_model.trainable = True\n",
        "        discriminator_model.compile(loss='binary_crossentropy', optimizer=opt_discriminator)\n",
        "\n",
        "        gen_loss = 100\n",
        "        disc_loss = 100\n",
        "\n",
        "        # Start training\n",
        "        print(\"Start training\")\n",
        "        for e in range(nb_epoch):\n",
        "            # Initialize progbar and batch counter\n",
        "            progbar = generic_utils.Progbar(epoch_size)\n",
        "            batch_counter = 1\n",
        "            start = time.time()\n",
        "\n",
        "            for X_real_batch in data_utils.gen_batch(X_real_train, batch_size):\n",
        "\n",
        "                # Create a batch to feed the discriminator model\n",
        "                X_disc, y_disc = data_utils.get_disc_batch(X_real_batch,\n",
        "                                                           generator_model,\n",
        "                                                           batch_counter,\n",
        "                                                           batch_size,\n",
        "                                                           noise_dim,\n",
        "                                                           noise_scale=noise_scale,\n",
        "                                                           label_smoothing=label_smoothing,\n",
        "                                                           label_flipping=label_flipping)\n",
        "\n",
        "                # Update the discriminator\n",
        "                disc_loss = discriminator_model.train_on_batch(X_disc, y_disc)\n",
        "\n",
        "                # Create a batch to feed the generator model\n",
        "                X_gen, y_gen = data_utils.get_gen_batch(batch_size, noise_dim, noise_scale=noise_scale)\n",
        "\n",
        "                # Freeze the discriminator\n",
        "                discriminator_model.trainable = False\n",
        "                gen_loss = DCGAN_model.train_on_batch(X_gen, y_gen)\n",
        "                # Unfreeze the discriminator\n",
        "                discriminator_model.trainable = True\n",
        "\n",
        "                batch_counter += 1\n",
        "                progbar.add(batch_size, values=[(\"D logloss\", disc_loss),\n",
        "                                                (\"G logloss\", gen_loss)])\n",
        "\n",
        "                # Save images for visualization\n",
        "                if batch_counter % 100 == 0:\n",
        "                    data_utils.plot_generated_batch(X_real_batch, generator_model,\n",
        "                                                    batch_size, noise_dim, image_data_format)\n",
        "\n",
        "                if batch_counter >= n_batch_per_epoch:\n",
        "                    break\n",
        "\n",
        "            print(\"\")\n",
        "            print('Epoch %s/%s, Time: %s' % (e + 1, nb_epoch, time.time() - start))\n",
        "\n",
        "            if e % 5 == 0:\n",
        "                gen_weights_path = os.path.join('../../models/%s/gen_weights_epoch%s.h5' % (model_name, e))\n",
        "                generator_model.save_weights(gen_weights_path, overwrite=True)\n",
        "\n",
        "                disc_weights_path = os.path.join('../../models/%s/disc_weights_epoch%s.h5' % (model_name, e))\n",
        "                discriminator_model.save_weights(disc_weights_path, overwrite=True)\n",
        "\n",
        "                DCGAN_weights_path = os.path.join('../../models/%s/DCGAN_weights_epoch%s.h5' % (model_name, e))\n",
        "                DCGAN_model.save_weights(DCGAN_weights_path, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExF9V5Qwdai-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "\n",
        "backend=\"tensorflow\"\n",
        "dset=\"celebA\"\n",
        "generator=\"upsampling\"\n",
        "batch_size=32\n",
        "n_batch_per_epoch=200\n",
        "nb_epoch=400\n",
        "epoch=10\n",
        "nb_classes=2\n",
        "do_plot=True\n",
        "bn_mode=2\n",
        "img_dim=64\n",
        "label_smoothing=\"store_true\"\n",
        "label_flipping=0\n",
        "noise_scale=0.5\n",
        "use_mbd=\"store_true\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNolsYjGcsAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "#import argparse\n",
        "\n",
        "\n",
        "def launch_training():\n",
        "\n",
        "    # Launch training\n",
        "    train_GAN.train(batch_size,n_batch_per_epoch,nb_epoch,generator,model_name,image_data_format,img_dim,bn_mode,label_smoothing,label_flipping,noise_scale,dset,use_mbd,epoch_size\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "#    parser = argparse.ArgumentParser(description='Train model')\n",
        "#    parser.add_argument('--backend', type=str, default=\"theano\", help=\"theano or tensorflow\")\n",
        "#    parser.add_argument('--generator', type=str, default=\"upsampling\", help=\"upsampling or deconv\")\n",
        "#    parser.add_argument('--dset', type=str, default=\"mnist\", help=\"mnist or celebA\")\n",
        "#    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')\n",
        "#    parser.add_argument('--n_batch_per_epoch', default=200, type=int, help=\"Number of training epochs\")\n",
        "#    parser.add_argument('--nb_epoch', default=400, type=int, help=\"Number of batches per epoch\")\n",
        "#    parser.add_argument('--epoch', default=10, type=int, help=\"Epoch at which weights were saved for evaluation\")\n",
        "#    parser.add_argument('--nb_classes', default=2, type=int, help=\"Number of classes\")\n",
        "#    parser.add_argument('--do_plot', default=False, type=bool, help=\"Debugging plot\")\n",
        "#    parser.add_argument('--bn_mode', default=2, type=int, help=\"Batch norm mode\")\n",
        "#    parser.add_argument('--img_dim', default=64, type=int, help=\"Image width == height\")\n",
        "#    parser.add_argument('--noise_scale', default=0.5, type=float, help=\"variance of the normal from which we sample the noise\")\n",
        "#    parser.add_argument('--label_smoothing', action=\"store_true\", help=\"smooth the positive labels when training D\")\n",
        "#    parser.add_argument('--use_mbd', action=\"store_true\", help=\"use mini batch disc\")\n",
        "#    parser.add_argument('--label_flipping', default=0, type=float, help=\"Probability (0 to 1.) to flip the labels when training D\")\n",
        "\n",
        "\n",
        "#    assert args.dset in [\"mnist\", \"celebA\"]\n",
        "\n",
        "    # Set the backend by modifying the env variable\n",
        "    if backend == \"theano\":\n",
        "        os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
        "    elif backend == \"tensorflow\":\n",
        "        os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "    # Import the backend\n",
        "    import keras.backend as K\n",
        "\n",
        "    # manually set dim ordering otherwise it is not changed\n",
        "    if backend == \"theano\":\n",
        "        image_data_format = \"channels_first\"\n",
        "        K.set_image_data_format(image_data_format)\n",
        "    elif backend == \"tensorflow\":\n",
        "        image_data_format = \"channels_last\"\n",
        "        K.set_image_data_format(image_data_format)\n",
        "\n",
        "#    import train_GAN No longer required\n",
        "\n",
        "    # Set default params\n",
        "    d_params = {\"mode\": \"train_GAN\",\n",
        "                \"dset\": dset,\n",
        "                \"generator\": generator,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"n_batch_per_epoch\": n_batch_per_epoch,\n",
        "                \"nb_epoch\": nb_epoch,\n",
        "                \"model_name\": \"CNN\",\n",
        "                \"epoch\": epoch,\n",
        "                \"nb_classes\": nb_classes,\n",
        "                \"do_plot\": do_plot,\n",
        "                \"image_data_format\": image_data_format,\n",
        "                \"bn_mode\": bn_mode,\n",
        "                \"img_dim\": img_dim,\n",
        "                \"label_smoothing\": label_smoothing,\n",
        "                \"label_flipping\": label_flipping,\n",
        "                \"noise_scale\": noise_scale,\n",
        "                \"use_mbd\": use_mbd,\n",
        "                }\n",
        "\n",
        "    # Launch training\n",
        "    launch_training()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMArMNyOUfz0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}